\chapter{Introduction}

Changing one's mind is a process that happens very frequently as part of one's daily routine. Pushing the power button on a very old television that is covered in dust and seeing it getting turned on, one would change their mind and believe that the television is working. They used to think it is broken because it hasn't been used for decades, in which case we say that they believed that ``the television is broken''. That belief was part of their state of mind. But seeing it work means that there is a contradicting belief ``the television works'' that needs to be adopted by the mind which implies removing the old belief that ``the television is broken''. Belief revision refers to the process of modifying the state of mind to adopt a new belief that contradicts existing beliefs.

Mind changing can be thought of as the manipulation of beliefs according to perception. When humans perceive any change in their world, they change their knowledge accordingly. So changing the knowledge (or beliefs\footnote{Throughout this study we will be using the terms ``knowledge'' and ``beliefs'' interchangeably, although they are not exactly the same. Knowledge is usually assumed to be a special kind of beliefs. However, for convenience, when we use the word ``knowledge'' we will be referring to ``belief.'' }) of an agent\footnote{The word ``agent'' here means humans, computers, or any thing that has knowledge and perception.} can be seen as what we informally call changing the mind of the agent. One example of belief change is \textbf{Revision}, which involves changing the knowledge base in order to add a conflicting belief. This can actually be broken down into two processes: changing the knowledge base to account for the conflict, and adding the new belief. 

The first process involves removing the beliefs that are causing that conflict. This process is called \textbf{Belief Contraction}. The second process involves adding the new belief and expanding the knowledge base accordingly. This process is called \textbf{Belief Expansion}. Contraction is the type of change we are mainly concerned with in this study. Kernel contraction is one approach to contraction that works by computing all kernels and removing some statements from each of them. 

\section{Kernel contraction by example}
The following chapters will explain kernel contraction in more detail, but it is helpful to get a brief idea on what it is before moving on. A kernel is a minimal set of beliefs that imply a certain belief. If our knowledge contains the following beliefs:
\begin{itemize}
\item We are in Canada
\item The temperature is 30 Celsius
\item If temperature is 30 Celsius in Canada, then it is summer,
\end{itemize}
we can infer that it is indeed summer time. In this case, the three beliefs together form a kernel that implies ``it is summer.'' That kernel can be used for contraction. If we want to give up the belief ``it is summer,'' we can remove a belief from the kernel so that the remaining two are not enough to imply that it is summer. In some other cases we can have more than one kernel, and the same rules can still apply: remove a belief from each kernel in order to perform kernel contraction. The beliefs used in this example are quite subjective, but the aim is to give a brief and simple example. The following chapters will talk more formally about kernel contraction.

\section{The language}
The knowledge of an agent can be represented as a set of beliefs. An agent is assumed to believe in $A$ if $A$ is a member of its \textit{belief set}\footnote{Belief sets will be explained in the next chapter.}. In this study, the language used to represent belief sets is \textit{Description Logic}. Description Logic is a family of formalisms that are used to represent knowledge. They use concepts to represent classes of individuals and roles to represent relationships between them. They have different expressive powers and different reasoning mechanisms with different complexities. They vary according to the set of logical operators they use. Here we use $\mathcal{EL}$ \cite{el}, which is a member of the Description Logic family. In \cite{contract}, an algorithm that contracts $\mathcal{EL}$ TBoxes was introduced. Our approach in this study is different mainly because it uses kernels to perform contraction.

\section{Scope of this study}
We aim at studying implementations of kernel contraction for knowledge bases represented in the description logic $\mathcal{EL}$. We will look at some basic implementations as well as some advanced ones. Our focus is on investigating the algorithmic aspects of those different approaches. The time complexity will be taken into consideration, and more importantly the optimality of the solutions found. Optimality can be measured in different ways, among them is one that makes more sense as a solution to humans and that is closer to solutions humans generally prefer.

The starting point of optimality seeking in this study will be from a syntactic point of view based on the hitting set problem. The hitting set problem is simply the problem of finding the smallest set of elements that intersect with each set of a given set of sets of elements (this will be explained in more details later). We adopt the hitting set approach because what we have is a set of kernels (sets of elements) that we need to remove one element from each. By doing this we hope to achieve syntactical optimality that is only measured by the size of the solution: the smaller the better. 

One of the significant contributions of this study is the investigation of algorithms that perform kernel contraction achieving semantic optimality based on $\mathcal{EL}$ semantics. These approaches exploit the structure of $\mathcal{EL}$ knowledge bases to find most reasonable solutions. Both approaches that seek semantic and syntactic optimality can be combined together to achieve better solutions. In contracting big knowledge bases, we can end up with kernels that include beliefs that are not all directly related to each other. In that case, we say that it makes more sense to select beliefs that are related to be removed rather than selecting the beliefs that are not. This idea motivates the heuristic that we call \textit{localization}. Another heuristic we introduce is \textit{specificity}, which is based on the idea that removing beliefs about specific matters is more reasonable than removing beliefs about general ones. We will study these two heuristics, which can be used to make decisions on which beliefs to remove based on the meaning of the logical operators and the structure of the language. These heuristics can be useful in cases where other approaches (such as the minimum hitting set algorithm) result in more than one solution with the same size, to select the solution that is more reasonable based on the meaning of the beliefs. Approaches that only consider the size of the solution do not always make a meaningful choice on which beliefs to remove. So we can then use the heuristics as a tie-breaker step when we get more than one solution.

We start the study by discussing the basic types of belief change, but before that, we build a ground for them by defining the framework that was introduced by Gardenfors. We define epistemic states and attitudes that will help in understanding the mechanics of belief change. We then explain some postulates introduced in the AGM framework; those postulates are considered rationality rules for belief change operations. Then we explain what description logic is and what logical operators are used. In that discussion, we focus on the most relevant variation to this study, which is $\mathcal{EL}$. 

After that, we will go over some belief contraction techniques and show how syntactic optimality can be achieved, and what the cost can be (in terms of time complexity.) We will talk more formally about contraction using kernels. Our new approach to kernel contraction is by reducing the problem of contraction to a graph problem. We will give a brief introduction to this new technique and will describe it in detail using a few different examples. We will use the algorithm for network flow problems to produce a solution to kernel contraction that will be guaranteed to be smallest. This algorithm cannot be used for every $\mathcal{EL}$ knowledge base; it only works on knowledge bases in some certain settings. This will be discussed in more detail later, and the examples will show the cases in which the graph technique is useful and the cases in which it is not. Then, we will turn to the approaches that consider the semantics of the language in finding best solutions to the kernel contraction problem. These heuristics are called \textit{Localization} and \textit{Specificity}. Later, we will see what they are and how useful they can be.