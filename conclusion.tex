\chapter{Conclusion}
We have discussed that the AGM framework introduces a reasonable set of rules that can be followed to reach rationality in building belief systems. However, we used a different formalism to express knowledge and perform contraction throughout this study than the one the AGM framework uses. This formalism, which is $\mathcal{EL}$, belongs to a family called Description Logic (DL). $\mathcal{EL}$ is one of the simple members of the DL family, and, as the rest of the DLs, uses concepts and roles as the building blocks of the knowledge base. We only showed how to perform contraction on TBoxes, and not ABoxes. We then discussed some basic approaches to contraction and introduced a general algorithm (Contraction Algorithm) that can accommodate the use of different heuristics to seek optimality. 

We introduced a restricted contraction algorithm that uses graphs and solves contraction as a network flow problem. The graph approach is restricted in the sense that it can be used only in certain cases where inference in the TBox involves only the transitive property of the subsumption relation. We showed how the approach is sound and complete (only in the restricted cases) by showing that it follows the four rationality postulates. We also discussed three heuristics, two of which are based on the semantics of the subsumption hierarchy of $\mathcal{EL}$, which are Localization and Specificity. We showed also how the greedy approach for contraction can be used.

The bottleneck for the contraction algorithms we discussed is actually generating all the kernels of a specific belief. The pinpointing algorithm we use for this purpose can sometimes take exponential number of steps with respect to the size of the TBox. Other than that, all the algorithms we discussed take polynomial time to run. 

TBox kernel contraction can be implemented to find the smallest set of beliefs to be removed in polynomial time using the graph approach. The graph approach works only when the inference involves only the subsumption relationship, not the existential quantification. The complexity of the algorithm is the same as the network flow algorithm complexity as it uses the concept of minimum cut to compute the kernels.

Specificity is also one of the main contributions of this study. The $\mathcal{EL}$ description logic sets up a hierarchical relationship between concepts that reflects a specificity relationship. A concept can be said to be more (or less) specific compared to another concept based on the subsumption relationship in the TBox. This inspires the solution we adopted in removing knowledge about more specific concepts before considering more general ones.

Two things are worth mentioning and could be addressed in some future work. First, contracting a TBox by $A \sqsubseteq B \sqcap A \sqsubseteq C$, which is equivalent to contracting by $A \sqsubseteq B \sqcap C$, can be done by contracting the TBox by only one of the two GCIs. However, the result of contracting by $A \sqsubseteq B$ will probably be different from the result of contracting by $A \sqsubseteq C$. So how can we choose which GCI of the two to contract by? It could be based on the side effects that each of them will cause to the TBox.

Second, our graph approach works by finding the minimum cut after computing the maximum flow. What if there is more than one cut? It would be useful if we use some algorithms that find all minimum cuts and apply heuristics to select the most appropriate cut to be removed. The current version of the algorithm we introduced is agnostic about the semantics of the minimum cut produced. But if we can combine it with some heuristics (e.g. specificity), we might get a more reasonable and more rational solution.

One addition that could be done in the future is to give an algorithm that contracts explicit knowledge in the ABox, not just the TBox. Also, the use of $\mathcal{EL}$ was very important to finding polynomial time contraction algorithms, based on the polynomial time subsumption algorithm. One other suggestion is to attempt to use other, more expressive, versions of DL and investigate if this results in higher order contraction than polynomial, e.g. extend the contraction algorithm introduced here to work with knowledge bases represented in $\mathcal{EL}^{++}$, which is a more popular and more expressive DL. Another intriguing question is: what advantage would using a more expressive DL give, and will it be worth it if contraction algorithms get harder and more complicated? 