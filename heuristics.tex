\chapter{Heuristics for contraction}
\label{heuristics}
In this study we care about approaching optimal contraction. In the context of contraction, we can define optimality to be removing the smallest number of beliefs, or removing beliefs that make more sense to be removed together. In the last chapter, we discussed contraction algorithms that attempt to achieve optimality by trying to remove the smallest number of beliefs in an attempt to adopt the concept of minimum change. However, we haven't considered the significance of the DL language or the subsumption hierarchy of $\mathcal{EL}$ in choosing the beliefs to be removed from a $TBox$. In this chapter, we look at heuristics that are motivated by the subsumption relationship between formulas. We will discuss ways of determining a preferred set of beliefs to be removed, based on the subsumption relationship of $\mathcal{EL}$ represented by the symbol $\sqsubseteq$. 

According to \textit{localization} and \textit{specificity} heuristics\footnote{These heuristics will be explained shortly.}, the preference of removing a certain set of beliefs is not based on their count. We can say that it is ``semantically'' better to remove beliefs that are related than to remove unrelated beliefs; by ``semantically'' we mean according to the meaning of the language's logical connectives. The semantics of DL language's logical connectives imply certain meanings to the knowledge expressed using it. Keeping such meaning in mind while contracting helps make a reasonable decision on what beliefs to remove. For example, it is arguably better to remove beliefs about specific concepts than general ones. Localization and specificity can be used as tie-breakers when we have multiple giveUpSets of the same size and we need to select only one. They can also be used on their own regardless of the size of the giveUpSets.

\section{Localization}
Giving up beliefs about a certain concept means being in doubt of this concept. The more beliefs we remove, the more concepts we become in doubt of. This can lead us to considering removing beliefs about a certain concept rather than beliefs about many different concepts. As discussed in \cite{zwei}, it is more efficient to remove beliefs that share concepts or roles in a TBox as it means being in doubt of fewer concepts.

This seems a reasonable approach as humans tend to deal with related beliefs more than unrelated ones. If we try to revise our knowledge, we will probably find ourselves leaning towards focusing on beliefs that share common concepts. 

The following $\mathcal{EL}$ example:
\begin{center}
(1) $AspergillusFumigatus \sqsubseteq Multicellular $ \\
(2) $Multicellular \sqsubseteq NotBacteria$ \\
(3) $AspergillusFumigatus \sqsubseteq Eukaryotes$ \\
(4) $Eukaryotes \sqsubseteq NotBacteria$ 
\end{center}
implies:
\begin{center}
(5) $AspergillusFumigatus \sqsubseteq NotBacteria$,
\end{center}
and we can see that there are two (5)-kernels:
\begin{center}
\{1, 2\} and \{3, 4\}
\end{center}

So, to contract the knowledge base by (5), we need to remove a belief from each kernel. However, removing (1) and (4) seems unreasonable because it means that we are in doubt of our knowledge of four concepts: $AspergillusFumigatus$, $Multicellular$ beings, $Eukaryotes$, and beings that are $NotBacteria$, while removing (1) and (3), on the other hand, makes us in doubt only of our knowledge of only three concepts: $AspergillusFumigatus$, $Multicellular$ beings, and $Eukaryotes$. Not only that, but removing (1) and (3) actually means that our knowledge of $AspergillusFumigatus$ is wrong; Our knowledge of $Multicellular$ beings and $Eukaryotes$ is not necessarily false. Same with removing (2) and (4), we suspect the soundness of our beliefs about beings that are $NotBacteria$.

Localization is implemented in \cite{zwei} using a graph-like approach. The algorithm operates by defining an edge-like relationship between formulas and using graph connectivity and graph clustering rules to determine localized sets. 

For every two GCIs $\mathbb{X}$ and $\mathbb{Y}$, we say that there is an undirected edge between them if they share a concept or role.
After applying that, we will end up with a graph-like structure, where some GCIs of the knowledge base are connected by edges. The most basic path in a graph is an edge, but a path can be composed of more than one edge. Generally, a path is a sequence of edges.

\begin{defn}(Conncected GCIs)
Two GCIs $\mathbb{A}$ and $\mathbb{B}$ are connected if and only if there is an undirected path from one of them to the other, i.e. if there is a sequence of edges and GCIs $\mathbb{A}e_1G_1e_2 ... G_{n-1}e_n\mathbb{B}$, where $G_i$ is a GCI, $e_i$ is an edge, every triple $G_ie_{i+1}G_{i+1}$ means $G_i$ and $G_{i+1}$ are connected by an edge, and $n \geq 0$.
\end{defn}

In the last $\mathcal{EL}$ example, we can remove one of the following sets to contract (5):
\begin{itemize}
\item \{1, 3\} (a)
\item \{1, 4\}. (b)
\end{itemize}

There are more possible giveUpSets, but (a) and (b) are sufficient to show how the definition of connected GCIs can be applied. 

For the set (a), there is an edge (hence, a path) between (1) and (3) because they share the same concept \textit{AspergillusFumigatus}. So, according to the definition, (1) and (3) are connected.

For the set (b), there is no path between (1) and (4) because they don't share any concept or role. So according to the definition, (1) and (4) are not connected.

\begin{defn}(Clusters)
A GCI $\mathbb{X}$ belongs to a cluster $\mathfrak{C}$ if and only if $\mathbb{X}$ is connected to every GCI in $\mathfrak{C}$.
\end{defn}

Thus, according to the definitions, the set (a) has only one cluster, as (1) and (3) are connected and belong to the same cluster. However, the set (b) has two clusters, because (1) and (4) are not connected in (b), so they belong to two different clusters.

The algorithm uses the notion of clusters to identify the best giveUpSet. A GCI belong to a cluster if it shares concepts or roles with other GCIs in the giveUpSet. If a giveUpSet contains a lower number of clusters than another set, then it is better to remove it during contraction. This is because the more clusters there is in a set the more sparse the graph is, hence the less localized the contraction is. 

The algorithm works on each possible giveUpSet separately. It builds the graph-like relation between formulas in the set according to the definitions we gave. It is important that the algorithm computes the connectivity in each set separately, because two formulas might have a path between them in one set but not the other; so they would be considered connected in a set and disconnected in the other. The algorithm then counts the number of clusters. It does that for all the sets, then chooses the set with least number of clusters to be removed.

\begin{algorithm}
\caption{Computing localized hit}
\label{LocalizeContraction}
\begin{algorithmic}[1]
\State // Compute the number of clusters in each set and choose the least
\Function{getLocalizedHit}{giveUpSets}
\State setWithLeastClusters $= null$
\State smallestNumber $= \infty$
\For{giveUpSet $\in$ giveUpSets}
\State numOfClusters $=$ getNumberOfClusters(giveUpSet)
\If{numOfClusters $<$ smallestNumber}
\State setWithLeastClusters $=$ giveUpSet
\State smallestNumber $=$ numOfClusters
\EndIf
\EndFor
\State \Return setWithLeastClusters
\EndFunction
\end{algorithmic}


\begin{algorithmic}[1]
\State // Compute the number of clusters
\Function{getNumberOfClusters}{giveUpSet}
\For{$i=1$ to size(giveUpSet)}
\State label$[i]=i$
\EndFor
\For{$i=1$ to size(giveUpSet)}
\State cluster$[i]=0$
\EndFor
\State // Label connected nodes (beliefs) similarly
\For{$i=1$ to size(giveUpSet)}
\For{$j=i+1$ to size(giveUpSet)}
\If{giveUpSet[i] and giveUpSet[j] are connected}
\State label$[j] = $label$[i]$
\EndIf
\EndFor
\EndFor
\State // Mark clusters based on used labels. Each label now represents a cluster
\For{$i=1$ to size(giveUpSet)}
\State cluster$[$label$[i]]=1$
\EndFor
\State // Calculate the number of clusters by counting the used labels
\For{$i=1$ to size(giveUpSet)}
\State numberOfClusters $=$ numberOfClusters $+$ cluster$[i]$
\EndFor 
\State \Return numberOfClusters
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{LocalizeContraction} selects the giveUpSet with least number of clusters by first computing the number of clusters in each giveUpSet and then choosing the one with the smallest number. The time complexity of the algorithm is $O(n^3 \cdot m)$, where $m$ is the number of kernels, $n$ is the size of the biggest kernel. The loop in the main function goes over the whole list of kernels, which is of size $m$, and the second function has a 3-level nested loop that goes over the kernel whose size is at most $n$ in two levels. The third level checks the connectivity of two nodes, which can take at most $n$ if it has to go over all the elements in the kernel.

\section{Specificity}

The subsumption hierarchy enforced by the subsumption relation in $\mathcal{EL}$ categorizes concepts into different levels of generality and specificity. Consider the kernel:% in Figure \ref{fig:AnimalKernel},
\begin{figure}[h]
\begin{center}
$Lion \sqsubseteq Mammal$\\
$Mammal \sqsubseteq Vertebrate$\\
$Vertebrate \sqsubseteq Animal$
\end{center}
\caption{Simple Animal kernel}
\label{fig:AnimalKernel}
\end{figure}


\begin{flushleft}
which implicitly entails
\end{flushleft}
\begin{center}
$Lion \sqsubseteq Animal$
\end{center}
And suppose we would like to contract the $TBox$ by $Lion \sqsubseteq Animal$. We have three options: we can remove $Lion \sqsubseteq Mammal$, $Mammal \sqsubseteq Vertebrate$, or $Vertebrate \sqsubseteq Animal$. Removing any of the three GCIs would guarantee minimum change. We can say that $Lion$ is more specific than $Mammal$ and $Mammal$ is more specific than $Vertebrate$. Removing $Lion \sqsubseteq Mammal$, in this case, is more reasonable than removing $Vertebrate \sqsubseteq Animal$, because $Lion \sqsubseteq Mammal$ involves concepts that are more specific than the ones involved in $Vertebrate \sqsubseteq Animal$. Removing $Vertebrate \sqsubseteq Animal$ may affect more concepts in the subsumption hierarchy (in worst case) than those affected by removing $Lion \sqsubseteq Mammal$. This means that removing $Vertebrate \sqsubseteq Animal$ means that all individuals that are $Vertebrates$ are no longer believed to be $Animals$, while removing $Lion \sqsubseteq Mammal$ only affects a subset of $Vertebrates$ which is $Lions$.

So, it is preferable to remove GCIs that involve specific concepts rather than removing GCIs that involve more general ones. To account for specificity, we assign a label to each of the concepts representing its level in the subsumption hierarchy, where levels increase with generality. And during contraction, we consider contracting GCIs that involve concepts at lower level (more specific) before considering GCIs that involve concepts at higher level.

Before explaining how the algorithm works, we need to explain the notion of \textit{children} that we use in the context of subsumption hierarchy. We use this notion to define the relationship between concepts that reflects a tree-like relationship between concepts. This will help us determine and compute specificity for each GCI.

Similar to the hierarchy of the subsumption graph of an $\mathcal{EL}$ TBox, we give a definition to \textit{parent} and \textit{child} concepts as follows:
\begin{defn}
\label{defn:Parent-Child}
For the GCI $A \sqsubseteq B$:
\begin{itemize}
\item $A$ is a \textit{child} concept relative to $B$, and
\item $B$ is a \textit{parent} concept relative to $A$.\\
\end{itemize}
\end{defn}

So by looking again at Figure \ref{fig:AnimalKernel}, we can now say that:
\begin{itemize}
\item Mammal is a \textit{parent} of Lion, and Lion is a \textit{child} of Mammal
\item Vertebrate is a \textit{parent} of Mammal, and Mammal is a \textit{child} of Vertebrate
\item Animal is a \textit{parent} of Vertebrate, and Vertebrate is a \textit{child} of Animal
\item Animal is the most \textit{general} concept, i.e. Animal subsumes all other concepts
\item Lion is the most \textit{specific} concept (least \textit{general}), i.e. it is subsumed by all other concepts
\end{itemize}

The approach we follow in this study for adopting the preference of removing sentences that contain more specific concepts is achieved by assigning weights to every GCI that reflects its generality. Then we select the kernels with minimal overall weight. Every GCI gets a numeric weight depending on the level of generality of the concepts involved in it (depending on their position in the subsumption graph). A GCI $A \sqsubseteq B$ gets weight `0' if $A$ has no children, i.e. if there is no rule of the form $X \sqsubseteq A$, where $X$ is an arbitrary concept; and the GCI involving its parent (A GCI of the form $B \sqsubseteq C$) will have weight `1' in that case. So the weight somehow represents the level in the subsumption graph, starting from level `0' at the trees' leaves (which is $A$ in this example.) For simplicity, we assume that the $TBox$ is acyclic -- so we avoid the problems that will be caused by loops.

Now we can compute the weights for the last TBox:

\begin{itemize}
\item $Lion \sqsubseteq Mammal$ has weight = 0
\item $Mammal \sqsubseteq Vertebrate$ has weight = 1
\item $Vertebrate \sqsubseteq Animal$ has weight = 2
\end{itemize}

The algorithm we introduce now removes kernels that contain GCIs involving most specific concepts by first computing the weights of every kernel based on the weights of the GCIs inside it. Following Definition \ref{defn:Parent-Child}, we start the algorithm by building the $children$ labels for all the concepts in the $TBox$ that we will use to assign weights to the GCIs. Given a $TBox$ $\mathcal{T}$, the algorithm proceeds as in Algorithm \ref{GraphBuild}.

\begin{algorithm}
\caption{Building the children graph}
\label{GraphBuild}
\begin{algorithmic}[1]
\Function{graphBuild}{T}
\State $children(X) = \emptyset$, for every concept X in$\mathcal{T}$
\For{every $A \sqsubseteq B \in T$}
\State $children(B) = children(B) \cup \{ A \}$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

Now we have a graph composed of nodes and their $child$ relationship; the nodes represent concepts and the edges (or the child relationship) are the subsumption relationship between them. So GCIs are represented by edges in the graph. The roots are now nodes that are not children of any node, and the leaves are nodes with no children. The edges involving leaves are assigned weight `0' and it increases by `1' every step towards the roots. So, the roots are most general concepts, and the leaves are most specific ones. If a GCI contains concepts $\alpha \sqsubseteq \beta$, where $\alpha$ has no children and $\beta$ has no parents, then it gets weight `0' according to Algorithm \ref{AssignWeights}. The weights are assigned to edges, not nodes. Given an edge $e$ that connects node $a$ to its child $b$, if $b$ has more than one child, the weight of $e$ is determined by the longest sequence of edges to a leaf starting at any of the children of $b$. So the GetMaxWeight function traverses a tree to all of its leaves computing the weight of each edge and choosing the weight according to the longest path to a leaf.

\begin{algorithm}
\caption{Assigning Weights}
\label{AssignWeights}
\begin{algorithmic}[1]
\Function {assignWeights}{T}
\For{every $A \sqsubseteq B \in T$}
\State $weight(A \sqsubseteq B) = GetWeight(A \sqsubseteq B)$
\EndFor
\EndFunction
\end{algorithmic}

\begin{algorithmic}[1]
\Function {getWeight}{$X \sqsubseteq Y$}
\If{$weight(X \sqsubseteq Y) \neq NULL$}
\State
\Return $weight(X \sqsubseteq Y)$
\EndIf
\If{$children(X) = \emptyset$}
\State
\Return 0
\Else
\State
\Return $1 + GetMaxWeight(X)$
\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}[1]
\Function {getMaxWeight}{X}
\State max = -- 1
\For{every Z in children(X)}
\State w = GetWeight($Z \sqsubseteq X$)
\If{ w $>$ max}
\State max = w
\EndIf
\EndFor
\State
\Return max
\EndFunction
\end{algorithmic}
\end{algorithm}

Now we have every GCI in $\mathcal{T}$ assigned a weight relative to its level of generality. So, in every kernel we have a preference level of what to give up. Based on the weights, we will choose the sentences with less weight over the ones with more weight to remove. This preference can be used as a tie-breaker after we apply the minimal hitting set algorithm and end up with more than one minimal set. If we arrive at two minimal hitting sets for the kernels, we can compute the overall weight of each of the hitting sets and remove the GCIs in the set with less overall weight. This is shown in Algorithm \ref{RemoveSpecific}. It uses the min-hit-CUT algorithm that gets the minimum hitting sets given a kernel set. There could be more than one minimum hitting set.

\begin{algorithm}
\caption{Removing specific hitting set}
\label{RemoveSpecific}
\begin{algorithmic}[1]
\Function {getMostSpecificHit}{kernelset}
\State min-hit-sets = min-hit-CUT(kernelSet)
\State $min = \infty$
\State hit = NULL
\State w(s) = 0, for every $s \in $ min-hit-sets
\For{every s in min-hit-sets}
\For{every $A \sqsubseteq B$ in s}
\State $w(s) = w(s) + weight(A \sqsubseteq B)$
\EndFor
\If{w(s) $<$ min}
\State min = w(s)
\State hit = s
\EndIf
\EndFor
\State
\Return hit
\EndFunction
\end{algorithmic}
\end{algorithm}

Now the CUT method in Algorithm \ref{MainAlgorithm} can be re-implemented using the algorithms introduced here to account for specificity heuristics. Given a TBox $\mathcal{T}$ and a GCI $\mathbb{A}$, the contraction algorithm is given in Algorithm \ref{MainAlgorithmModified}.

\begin{algorithm}
\caption{Contraction algorithm -- modified}
\label{MainAlgorithmModified}
\begin{algorithmic}[1]
\Procedure{contract}{$\mathcal{T}$, $\mathbb{A}$}
\State kernelset = \Call{pinpoint}{$\mathcal{T}$, $\mathbb{A}$}
\State graphBuild($\mathcal{T}$)
\State assignWeights($\mathcal{T}$)
\State giveUpSet = \Call{getMostSpecificHit}{kernelset}
\State$\mathcal{T}$=$\mathcal{T}$/ giveUpSet
\EndProcedure
\end{algorithmic}
\end{algorithm}

The contraction algorithms in this version guarantees the minimality, then specificity, of the removed beliefs. It guarantees the minimality by reducing the selection of beliefs from the kernelset to the minimal hitting set problem. The better the performance and the optimality of the hitting set algorithms, the efficient and minimal the contraction algorithm is. If the minimal set algorithm produces more than one solution of the same size, the specificity heuristic is used as a tie-breaker. It selects the solution with more specific beliefs to be removed.

The time complexity of Algorithm \ref{AssignWeights} is polynomial. The worst-case time complexity is $O(n \cdot l)$, where $n$ is the size of the TBox $\mathcal{T}$ and $l$ is the depth of the subsumption graph. Since the largest value for $l$ is $n$, we can consider the worst-case time complexity to be $O(n^2)$. The algorithm works as depth-first search because of the recursive call in $getWeight$ function. The $graphBuild$ function takes $O(n)$ steps to build the parent-child relationship graph, where $n$ is the size of the TBox $\mathcal{T}$. Finally, the $getMostSpecificHit$ function takes $O(n \cdot m)$ steps in the worst case, where $n$ is the size of the TBox $\mathcal{T}$, and $m$ is the number of kernel sets. So the contraction algorithm takes polynomial time if the minimum hitting set algorithm runs in polynomial time. But, if the minimum hitting set algorithm is not polynomial-time, then the contraction algorithm will not be. Thus, it is safe to say that the time complexity of the contraction algorithm is lower-bounded by the time complexity of the minimum hitting set algorithm's time complexity.